# Real Estate Listing Optimization

## ğŸ“Œ Overview
This project is the data crawling section of **Fellowship.AI (Sept 2025)**.  
The goal is to build an intelligent pipeline that scrapes property listings from **Zillow** and **Redfin**, parses structured data, and later runs AI-powered analysis for pricing, content quality, and optimization recommendations.  

The platform aims to help real estate agents identify **mispriced or poorly optimized listings** and provide **actionable improvements** to reduce time-on-market.

---


## ğŸ“‚ Project Structure
```
REAL-ESTATE-LISTING-OPTIMIZATION/
â”‚â”€â”€ config/
â”‚â”€â”€ data/
â”‚â”€â”€ crawl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ batch.py
â”‚   â”œâ”€â”€ extract_search.py
â”‚   â”œâ”€â”€ fc_extract_adapted.py
â”‚   â”œâ”€â”€ fetch.py
â”‚   â”œâ”€â”€ parse_detail.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ settings.py
â”‚â”€â”€ .env
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ configs.ipynb
â”‚â”€â”€ debug.ipynb
â”‚â”€â”€ README.md
```

---

## âœ… Current Progress

- **Schema-driven structured data**: Address, price, beds, baths, area, description, images, and more.
- **Batch pipeline**: Fetch â†’ Parse â†’ Run orchestration ([src/pipeline.py](src/pipeline.py)).
- **Raw HTML/JSON storage**: All source files saved for reproducibility.
- **Robust parsing**: Multi-strategy extraction from Redfin/Zillow, with schema.org and regex fallbacks. => now: parsed values return `null` due to mismatch between schema and actual site data.  
- **Configurable areas/zips**: Easily add new cities/zips via [config/listings_config.json](config/listings_config.json).
- **Next step**: implement robust parsing to map real Zillow/Redfin fields to schema.


---

## âš™ï¸ Usage
Run from the project root:

```bash
# Step 1: Initialize batch and fetch search pages
python -m src.batch

# Step 2: Fetch detail pages
python src/pipeline.py fetch --n 10

# Step 3: Parse details into structured JSON
python src/pipeline.py parse-details --limit 10

# Step 4: End-to-end run (fetch + parse)
python src/pipeline.py run --n 10
```

Outputs are stored in `data/batches/{batch_id}/`:

- `raw/` â€” raw HTML/JSON snapshots of listing pages
- `structured/` â€” parsed structured JSON files

---

## ğŸ“‘ Data Specification

- **Identifiers**: listing_id, platform_id, source_url, batch_id, scraped_timestamp
- **Address**: street, unit, city, state, postal_code
- **Property Attributes**: beds, baths, interior_area_sqft, lot_sqft, year_built, condition
- **Listing Info**: status, list_date, days_on_market, list_price, price_per_sqft
- **Media**: photos, videos, floorplans
- **Description**: textual content
- **Market Signals**: views, saves, share_count
- **Deduplication**: possible_duplicate, duplicate_candidates

See [`config/listings_config.json`](config/listings_config.json) for the full schema.

---
---

## ğŸš€ Next Steps

- Improve parsing logic for robust extraction of beds, baths, price, etc.
- Integrate Supabase for structured listing storage and media management.
- Expand schema coverage for additional property features and market signals.

---

## ğŸ“ References

- `src/pipeline.py` â€” main orchestration
- `src/fetch.py` â€” fetching logic
- `src/parse_detail.py` â€” parsing logic
- `config/listings_config.json` â€” schema and area/zips config

---

## ğŸ¤ Contributing

PRs and issues welcome! See `configs.ipynb` for config conventions and data spec.
